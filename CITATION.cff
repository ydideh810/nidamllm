cff-version: 1.2.0
title: 'Nidam: Operating LLMs in production'
message: >-
  If you use this software, please cite it using these
  metadata.
type: software
authors:
  - given-names: Aaron
    family-names: Pham
    email: aarnphm@jileml.com
    orcid: 'https://orcid.org/0009-0008-3180-5115'
  - given-names: Chaoyu
    family-names: Yang
    email: chaoyu@jileml.com
  - given-names: Sean
    family-names: Sheng
    email: ssheng@jileml.com
  - given-names: Shenyang
    family-names: Zhao
    email: larme@jileml.com
  - given-names: Sauyon
    family-names: Lee
    email: sauyon@jileml.com
  - given-names: Bo
    family-names: Jiang
    email: jiang@jileml.com
  - given-names: Fog
    family-names: Dong
    email: fog@jileml.com
  - given-names: Xipeng
    family-names: Guan
    email: xipeng@jileml.com
  - given-names: Frost
    family-names: Ming
    email: frost@jileml.com
repository-code: 'https://github.com/jileml/Nidam'
url: 'https://jileml.com/'
abstract: >-
  Nidam is an open platform for operating large language
  models (LLMs) in production. With Nidam, you can run
  inference with any open-source large-language models,
  deploy to the cloud or on-premises, and build powerful AI
  apps. It has built-in support for a wide range of
  open-source LLMs and model runtime, including StableLM,
  Falcon, Dolly, Flan-T5, ChatGLM, StarCoder and more.
  Nidam helps serve LLMs over RESTful API or gRPC with one
  command or query via WebUI, CLI, our Python/Javascript
  client, or any HTTP client. It provides first-class
  support for LangChain, jileML and Hugging Face that
  allows you to easily create your own AI apps by composing
  LLMs with other models and services. Last but not least,
  it automatically generates LLM server OCI-compatible
  Container Images or easily deploys as a serverless
  endpoint via jileCloud.
keywords:
  - MLOps
  - LLMOps
  - LLM
  - Infrastructure
  - Transformers
  - LLM Serving
  - Model Serving
  - Serverless Deployment
license: Apache-2.0
date-released: '2023-06-13'
